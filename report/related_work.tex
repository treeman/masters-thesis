\chapter{Related work}\label{cha:relwork}

A lot of research has been put into recommender systems \citep{bobadilla2013recommender, takacs2012alternating, lu2012recommender, bennett2007netflix, su2009survey}. Most articles are concerned with improving accuracy of recommender system results, such as minimizing \textit{Root of Mean Square Error} \prmse. This was the case for the popular Netflix Prize \citep{bennett2007netflix} which was concerned with recommending movies given user ratings for other movies. \textit{Explicit feedback} recommender systems continue to be a well researched area \citep{bobadilla2013recommender, takacs2012alternating, lu2012recommender, su2009survey}. \textit{Implicit feedback} systems, which is the focus of this thesis, have grown in popularity and are being actively researched but is still less researched than \textit{explicit feedback} \citep{hu2008collaborative, bobadilla2013recommender, takacs2012alternating}.

According to \citep{lai2012hybrid} the \textit{Top-N Recommendation problem} is the real problem of many on-line recommender systems and it is common to seek improvements for recommendation quality, using \textit{Precision}, \textit{Recall} or \textit{F-measure} \citep{bobadilla2013recommender, takacs2012alternating, herlocker2004evaluating}. This is also the focus of this thesis.

The 2nd Linked Open Data-enabled Recommender Systems Challenge
\footnote{2nd Linked Open Data-enabled Recommender Systems Challenge, 2015. \url{http://sisinflab.poliba.it/events/lod-recsys-challenge-2015/}}
is another competition which focuses on improving recommendation quality using \textit{F-measure} for the Top-N Recommendation problem as well as additional objectives such as \textit{diversity} \citep{bobadilla2013recommender}. The recommender system challenge uses \textit{explicit feedback} in the form of likes, but the data format is compatible with the model \eqref{eq:hist} this thesis uses. They use item meta-data such as genres, albums and actors which is not applicable to the general \textit{implicit feedback} system this thesis is focused on.

Together with the research many versions of different recommender systems have been implemented, with recommender systems becoming more and more popular \citep{bobadilla2013recommender, su2009survey}. One of the most popular types are hybrid recommender systems which combine different types of data and algorithms \citep{bobadilla2013recommender, lai2012hybrid}. This was the winning approach for the Netflix Prize which combined 107 different algorithms in different ways to produce the final recommendations
\footnote{ Netflix: Recommendations beyond 5 stars (Part 1), 2012. \url{http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html} }.

Optimization strategies for parameter tuning differ depending on the algorithm.  Alternating least squares \pals is a popular recommendation algorithm used both in \textit{explicit feedback} and \textit{implicit feedback} systems \citep{hu2008collaborative, takacs2012alternating}. Stochastic gradient descent \psgd is a popular optimization strategy for \als \citep{hu2008collaborative, takacs2012alternating} but there is also a custom optimization strategy purely for \als \citep{takacs2012alternating}. Another popular approach is bayesian personalized ranking which can also be optimized with \sgd \citep{rendle2009bpr}.

No literature concerning parameter optimization could be found for either \textit{link-analysis} nor \textit{katz-eig}.  Grid search seems to be the recommended approach for optimizing hyperparameters
\footnote{Recommended by Andrew Ng for the course Machine Learning, Stanford. \url{https://class.coursera.org/ml-006}}
. For \textit{implicit feedback} systems the optimization of common cost functions
\footnote{Similar to the definition in \eqref{eq:simple_cost}.}
is computationally expensive \citep{takacs2012alternating}.

The \textit{link-analysis} algorithm compared favorably in recommendation quality by \citep{huang2007comparison}. But without any analysis of the algorithms' parameters. The parameter values are simply stated but not commented on any further. No literature with further comments on the parameters could be found. Relatively poor runtime performance is noted \citep{huang2004link} but no actual comparisons are found.

Similarly \textit{katz-eig} had some positive recommendation quality results \citep{shin2012multi} but without parameter analysis and no mention of the algorithm's speed. No literature for parameter tuning could be found.


%\subsection{Cleanup}

%\citep{lu2012recommender}:
%* Comparison and evaluation of approaches. With respect to explicit feedback with ratings.
%* Comments on SVD.

%\citep{tintarev2007survey}:
%* Comments on the importance of explaining recommendations

%Maybe not that interesting/relevant.
%\citep{resnick1997recommender}:
%* Some (old) examples of recommender systems and their features.

%\citep{herlocker2004evaluating}:
%* Precision/Recall/F1. Novelty/Confidence

%\citep{bennett2007netflix}:
%* Explicit feedback!

%\citep{takacs2012alternating}:
%* ALS for implicit feedback systems
%* Mentions explicit feedback
%* Naive minimzation of the objective function is expensive. Usually overcome by a trade-off between the accuracy for computational efficiency by sampling the objective function
%* Uses a variant of the classical cost function. Models 0 as negative and 1 as positive
    %This model has some faults
    %0. A non-purchase is treated as a negative, but it's not necessary
    %1. A purchase (1) can still be rated negatively
    %2. The user may purchase the item for someone else
%* Also uses a rank based objective function
%* Mentions that implicit feedback isn't as explored
%* ALS is a popular approach for implicit feedback
%* BPR (Bayesian Personalized Ranking) is another popular approach
    %Use stochastic gradient SGD descent on a ranking based objective function!
%* ImplicitALS uses ALS for training instead of SGD
%* More comments on ranking evaluations

%\citep{su2009survey}: A survey of collaborative filtering techniques
%* Examples of explicit feedback things (has many examples!)

