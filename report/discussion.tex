\chapter{Discussion}\label{cha:discussion}

\textit{Can free form sections here}

\section{Recommender systems}

\textit{Overall design of recommender systems and some future work for the one I designed}

The recommender system developed in this thesis is very bare bones compared to some larger commercial examples. It's reasonable as the purpose was to produce a first prototype, or a core, for Comordo Technologies to built upon and extend.  Larger examples of integrated recommender systems include Netflix, Spotify, Youtube and Amazon.

The list of features of a modern recommender system could have is large. An important feature is the act of explaining recommendations, which can be seen in both Netflix and Spotify. Netflix notes
\footnote{
Netflix: Recommendations beyond 5 stars (Part 1), 2012.
\url{http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html}
}
that explanations improve trusts in the recommender system and makes the recommendations more effective. This is also supported in the literature where it is noted that good recommendations should be accompanied with an explanation \citep{hu2008collaborative}.

The intuition is if the recommendations come with a note ``Recommended to you because you watched Die Hard'' and if you liked Die Hard you're more likely to value the recommendations.

Extending Comordo's recommender system with explanations would be a good direction for future work. Developing algorithms which examine the linked structure could be a possible way of generating explanations, this approach would be especially suited for \textit{link-analysis} but it might also be possible for \textit{katz-eig}. Using a different set of recommendation algorithms with explicit support for explanations might be another possible approach.

Another feature which has gained some popularity lately \citep{bobadilla2013recommender} is diversifying the recommendations. For Netflix it is common that a user account not just represents a single person but a whole household which is why Netflix values diversity in their recommendations
\footnote{
Netflix: Recommendations beyond 5 stars (Part 1), 2012.
\url{http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html}
}.
Diversity is the act of recommending different types of items, in the Netflix example instead of only recommending action movies the recommendations include different genres like drama, animated or horror. This isn't just relevant when several persons share one account but it also accommodates for different moods of a single person.

The aptly named \textit{Diversity} \citep{bobadilla2013recommender} is a metric, similar to \textit{F-measure}, which measures the diversity of the recommendations
. In conjunction with \textit{F-measure}, \textit{Diversity} was the evaluation metric used for the 2nd Linked Open Data-enabled Recommender Systems Challenge
\footnote{
Evaluation | 2nd Linked Open Data-enabled Recommender Systems Challenge, 2015.
\url{http://sisinflab.poliba.it/events/lod-recsys-challenge-2015/tasks/evaluation-service/}
}.

Including considerations for diversity into Comordo's recommender system is another good direction for the future, possibly by optimizing for \textit{Diversity} instead of \textit{F-measure}. It's also something Comordo's clients have mentioned.

The distinction between offline and online recommender systems is something not considered in the recommender system design by this thesis, but it's an important one. Instead of generating recommendations on a fixed schedule, like once a day, it is desirable to generate recommendations in real time. The way Netflix does it
\footnote{
Netflix: System Architectures for Personalization and Recommendation, 2013.
\url{http://techblog.netflix.com/2013/03/system-architectures-for.html}
}
is to use faster and simpler algorithms for generating recommendations in real time and more complex algorithms for offline recommendations which take longer to run.
As the complexity of the system grows the line between offline and online algorithms become more blurred and more layers can be added. Netflix has an intermediate layer between the online and offline for example.

When Comordo finalizes their remote API the online/offline distinction should be thought about. I imagine functionality for generating recommendations in real time, so a consumer at the e-commerce website can get instant feedback, would be appreciated by their clients.


\subsection{Implementation}

\textit{Some comments on the implementation}

One of the purposes of this thesis was to lay the foundation of Comordo Technologies' recommender system and a goal was to make it so that the recommender system could easily be extended. Input of data into the system was accomplished by a plugin system which could easily be extended to read data of different formats. A plugin system designed in this way is very flexible as it can accomodate different input and output formats with little programming effort. Input in a standard form is also easy to accomodate, for example an interaction matrix already created and stored in a ``.mat'' data file.

%A further extension could be to use the same approach for the exporter module. The longterm goal for Comordo is to create a REST based API which makes the extension a lower priority. It could still be useful if Comordo's clients wants to test the recommendations Comordo can give them by offline processing of interaction data.

%In the future the functionality of the exporter module could be replaced by either the remote API or the admin web interface.

In the future the filtering functionality in the reader module could be moved to an external tool operating directly on the database. This would allow for a more efficient reading of the interaction data by streaming it directly into the database. The current implementation loads it all into memory and thereafter populates the database, which might cause memory problems if large data is given.

%Extensions to allow for metadata about the interactions could be beneficial. Filtering away very old interactions or the ability to split the training and test sets depending on the interaction date can be very useful.


\section{Datasets}

\textit{alpha} and \textit{alpha2} were too large to be run on my test setup due to low memory. \textit{eswc2015movies} and \textit{eswc2015music} were not runnable with \textit{link-analysis} as Matlab crashed for unknown reasons when attempts were made. My suspicion is on memory problems there as well. A better setup will be needed to be able to process datasets such as these, or a filtering of the datasets to smaller size would be needed. Optimization of the algorithms with respect to memory usage were not done and improvements could be made.


The datasets were examined both with regards to the number of interactions and examined if there were any clusters. \textit{alphaS} was an outlier as it had many users (51\%) with only one interaction. To generate recommendations a list of popular items could be provided and the users could be removed from the training and test sets for a general speedup and keeping the relative recommendation quality as high or possibly higher. The many one-item users explains the requirement for 61\% of all users to be able to reach 95\% of the users in \tableref{tab:top_data}.

The sparsity of data in \textit{alphaS} explains the better recommendation accuracy for \textit{link-analysis} compared to \textit{katz-eig} (\tableref{tab:alg_full_perf}) as \textit{link-analysis} was specifically designed to solve the sparse data problem \citep{huang2004link, huang2007comparison}. Similar results can be seen for \textit{eswc2015books} which supports the claim in the literature.

The optimal parameters for \textit{eswc2015books} are fairly strange. For \textit{katz-eig} $K = 1$ gives the best recommendation quality, but that means the recommendations are given by a rank-1 approximation.
\Warning[TODO]{ Why? }

Also for \textit{link-analysis} $\gamma = 0$ gives the best recommendation quality, which means that no penalizing users with many purchases occur. This could be explained by the low variance of the users interaction count. All users have a very similar amount of interactions and the point of $\gamma$ was to penalize the user representativeness score $UR(u, \hat{u})$ if $u$ had more interactions than $\hat{u}$. If they have a similar amount, it doesn't matter.


\section{Evaluation}

Different types evaluation metrics have been used throughout the literature. The one used in this thesis, \textit{F-measure}, is a popular one but other metrics could yield different results. As mentioned before \textit{Diversity} is another popular metric.  \textit{Rank Score} \cite{huang2007comparison} is another metric designed to rate a ranked list, list with the most recommended items, which is what is evaluated in thesis. \textit{Rank Score} and \textit{F-measure} was used to evaluate \textit{link-analysis} \cite{huang2007comparison}. For future work it would be interesting to evaluate \textit{link-analysis} and \textit{katz-eig} using \textit{Diversity}.


%The evaluation metric of \textit{F-measure} can be questioned/discussed...? Why no cost function? Probably would be better (if could be found, but can it?) ?

%Alternative evaluation metrics. \rmse, rank, accuracy, diversity.

%A common deficiency for evaluation metrics is the lack of formalization. The metrics themselves are well defined but implementation details differ and are sometimes missing which can lead to different results between similar experiments.


%\section{Parameter analysis}

%Referenced article introduces $\eta = 1$, but treats it as a constant. No further comments about it!

%Same article: $\gamma$ is said to be in the range 0 to 1, but I found that larger ones are good and often even better.


\section{Optimization}

\textit{Talk about the different optimization techniques here}

\subsection{link-analysis}

The paper introducing \textit{link-analysis} \cite{huang2004link} does not include $\eta$ in the description. It is instead introduced by the same authors in a later paper \cite{huang2007comparison} but there it is just set to $\eta = 1$ without further comments or analysis.

The analysis in \sectionref{sec:param:link} shows that $\eta = 1$ is a well performing value. The modelling of \textit{link-analysis} supports it as the most logical value, which probably is why they chose it. When $\eta > 1$ very similar results as $\eta = 1$ are obtained. This can be explained by the algorithm description. The purpose of $\UR_0 = \eta * I_M$ is to keep a high user representativeness score for the user itself. It doesn't matter that much how high the value is, as long as it's higher than the surrounding values. With the normalization step included in the iterations a value of $\eta \geq 1$ makes sure of that. Worth pointing out is that the optimal parameters can have $\eta > 1$, but it's not with a very big margin. Inaccuracy in multiplications is a plausible explanation.

More interesting and harder to explain is values $\eta < 0$, which can be even better than $\eta > 0$. I have come to no reasonable explanation and it might be worth investigating further.

The same papers \cite{huang2004link, huang2007comparison} mentions that investigations with $0 \leq \gamma \leq 1$ have been made and then a value of 0.9 was a good value. My investigations include a larger interval and optimal values for $\gamma$ can be found outside that range, \textit{romeo} has a local maxima with $\gamma = 2$. The findings in this thesis however do support the claim that 0.9 is a reasonably good value.

Analysis of $\gamma$ in \sectionref{sec:param:link} suggests that \textit{F-measure} is almost convex with a clear ``hill'' in the function space. There are small local maxima however but as a whole it follows a clear hill form. This suggests that a hill climbing optimization strategy might perform well for these datasets. It might also be worthwhile to keep $\eta$ fixed as was done in \cite{huang2004link, huang2007comparison}.

This is supported by the tests in \sectionref{sec:tuning:link} which show that an adaptive hill climbing algorithm produce recommendations almost as good as a full blown grid search in only a fraction of the time needed. The difference in recommendation quality between optimizing over both $\gamma$ and $\eta$ compared to just optimizing over $\gamma$ while $\eta = 1 \text{ or } -1$ is small, but the runtime difference is not.

\subsection{katz-eig}

The impact of varying $\beta$ was very small, as shown in \sectionref{sec:param:katzeig}. It's surprising as according to the model $\beta$ represents the link dampening. Another surprising result is that the training curves in \sectionref{sec:training:katz} show that the number of iterations does not have a sizable effect on \textit{F-measure}. There is some improvement, but it's a very minor one.

These results suggests that \textit{katz-eig} gives very similar recommendations as directly using a SVD-$k$ approximation would, which is what happens when $\beta = 0$. The number of iterations intuitively represents how many links are followed, but if the recommendation quality is the same regardless of the number of iterations, then only one link followed is enough. Recommendations based on SVD is an acknowledged way of generating good predictions \citep{bobadilla2013recommender} so the assumption is reasonable.

%These results suggests that \textit{katz-eig} could in fact be replaced by a rank $k$ SVD approximation. This is what happens when $\beta = 0


\vspace{1cm}

Optimization technique interesting to try is bayesian optimization!

Clustering would be nice to explore.

The lack of an easy cost function to optimize against is a hinderance. In practice calculating the top-10 recommendations is a slow process and a faster way to evaluate the recommendation quality given a set of parameters would give a nice speed improvement.

Future work: Bayesian optimization and simulated annealing.


\section{Algorithm comparisons}

\textit{Draw overall conclusions of the different algorithms}

\textit{katz-eig} has much better runtime performance compared to \textit{link-analysis} and generates comparable recommendation quality. \textit{link-analysis} have slightly better quality for sparse datasets, which is supported by the literature. In fact the primary motivation for \textit{link-analysis} is to generate recommendations for sparse data. But still, \textit{katz-eig} gives comparable performance.

Optimization is very expensive, but generating recommendations for the full datasets for both algorithms are fairly fast. They can both be useful commercially when run offline.


\section{All the rest...}

\begin{enumerate}
    \item ??
\end{enumerate}

\input{discussion/results.tex}
\input{discussion/method.tex}

