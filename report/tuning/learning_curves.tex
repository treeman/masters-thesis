
\section{Learning curves}\label{sec:graphs:learning_curves}

Learning curves plots the evaluation metric with respect to a varying training set size. The expectation here is that the algorithms should fit better the bigger the training set size becomes, given the same test set. The algorithms should produce better recommendations the more data they have to learn from. Learning curves is a good way to see if the learning process work like it is supposed to.

The evaluation uses the training matrix $A_{train}$ and the test set matrix $A_{test}$. For each step a random selection of a specific size is selected from $A_{train}$, recommendations are generated and evaluated against the same test set $A_{test}$. The dimensions of the matrices will the same, only the number of non-zero elements are increased with the training set size. This is done 10 times for each training set size as to remove variations from the random selection. Ideally more repetitions should be done but due to time constraints they were not.

Optimized parameters as described in \appendixref{app:opt_params} are used. The plots also describe the standard deviation.

\twopic{fig/learning_curves/alphaS_learning_perf.png}{fig/learning_curves/alphaS_learning_time.png}{
\textit{alphaS}
}
\twopic{fig/learning_curves/eswc2015books_learning_perf.png}{fig/learning_curves/eswc2015books_learning_time.png}{
\textit{eswc2015books}
}
\twopic{fig/learning_curves/movielens_learning_perf.png}{fig/learning_curves/movielens_learning_time.png}{
\textit{movielens1m}
}
\twopic{fig/learning_curves/romeo_learning_perf.png}{fig/learning_curves/romeo_learning_time.png}{
\textit{romeo}
}
\FloatBarrier

As the recommendation performance increase with increasing training size the learning process can be said to be working.

A notable observation about the runtime is that the runtime for \textit{link-analysis} increases almost linearly with the increase in training set size, the runtime for \textit{katz-eig} is almost indifferent. This is to be expected as \textit{katz-eig} operates on a low-rank approximation of the interaction matrix $A_{train}$ while \textit{link-analysis} operates directly on the matrix. The sparse matrix format (\sectionref{sec:background:theory:model}) which discards zero elements during calculations is computationally more complex as the sparsity decreases.

Another observation is that \textit{link-analysis} seems to generate better recommendations for the sparse datasets \textit{alphaS} and \textit{eswc2015books} and also for \textit{movielens1m} and \textit{romeo} a short while when the training size was smaller.

The large variation in \textit{F-measure} for \textit{katz-eig} for \textit{alphaS} and \textit{eswc2015books} can be explained by the sparsity of the datasets. As the training set is randomly selected for each run up to a selected number interactions, it is possible that a large number of users with very few interactions are selected, which makes for a bad basis for recommendations. This is especially true for \textit{alphaS} where most of the users only have one interaction.

The same variation is not seen with \textit{link-analysis} as the recommendations are produced on a link-following basis instead of trying to blur together users like \textit{katz-eig} does.

\FloatBarrier

