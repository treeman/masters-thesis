
\subsection{Supervised learning}\label{sec:background:theory:suplearn}

The task of \textit{supervised learning} is given a \textit{training set} with input-output pairs discover a function, the hypothesis, which approximates the input-output mapping.  To measure the accuracy of the hypothesis match it against a \textit{test set} with input-output pairs distinct from the training set.
\citep{norvigAI}

There can be multiple available models for the hypothesis, for example if the hypothesis is a polynomial function of the form 

\begin{equation}
f(x) = a_n x^n + a_{n - 1} x^{n - 1} + ... + a_2 x^2 + a_1 x + a_0
\end{equation}

then the polynomial degree $n = 1, 2, 3, ...$ represents different possible models for the hypothesis \citep{norvigAI}. Other examples include the number of layers and the number of units in a neural network \footnote{Machine Learning, Stanford. \url{https://class.coursera.org/ml-006}} or the rank of a low rank approximation \footnote{katz-eig models this way, see \sectionref{sec:background:theory:katzeig}}.

The different models represents the complexity of the hypothesis. A more complex model can make a better fit to the training data but that introduces the problem of \textit{overfitting} where the hypothesis fits the training data \textit{too} well and it will not fit the test data.
\citep{norvigAI}

\textit{Model selection} is thus performed by evaluating performance with a \textit{validation set}. The reason not to both choose the model and evaluate the model using the test set is that then we will have overfit the test set as we both choose the best model and then evaluate with \textit{the already best fit}. \citep{norvigAI}

The recommended ratio to split the training, validation and test set differs but common recommendations include 60/20/20, 80/10/10, or 70/15/15 \footnote{As recommended by Andrew Ng, Stanford. \url{https://class.coursera.org/ml-006}} depending on domain and the size of the available data set. It is important that the sets are pairwise disjunkt.

If there is no need for a validation set, which can be the case if there are no models to choose from, common training/test set ratios include 70/30, 80/20 or 90/10 \cite{hu2008collaborative, norvigAI} \footnote{Andrew Ng also mentions these values}.

In summary machine learning for supervised learning is done in a couple of steps:

\begin{description}
    \item[Preface] Split data set into training, test and validation sets.
    \item[Training phase] Train the hypothesis using the training set.
    \item[Model selection] Select model using the validation set. (Optional)
    \item[Evaluation] Estimate the accuracy using the test set.
    \item[Application] Apply the developed model to real world data and get results.
\end{description}

Another way to combat overfitting is with \textit{regularization}. Regularization searches for a hypothesis which directly penalizes complexity.  Regularization still needs to select the hyperparameter $\lambda$ using model selection.
\citep{norvigAI}
