
\subsection{Supervised learning}\label{sec:background:theory:suplearn}

The task of \textit{supervised learning} is given a \textit{training set} with input-output pairs discover a function, the hypothesis, which approximates the input-output mapping.  To measure the accuracy of the hypothesis match it against a \textit{test set} with input-output pairs distinct from the training set.
\citep{norvigAI}

There can be multiple available models for the hypothesis, for example if the hypothesis is a polynomial function of the form 

\begin{equation}
f(x) = a_n x^n + a_{n - 1} x^{n - 1} + ... + a_2 x^2 + a_1 x + a_0
\end{equation}

then the polynomial degree $n = 1, 2, 3, ...$ represents different possible models for the hypothesis \citep{norvigAI}. Other examples include the number of layers and the number of units in a neural network or the rank of a low rank approximation.
\Warning[TODO]{ examples need ref }

The different models represents the complexity of the hypothesis. A more complex model can make a better fit to the training data but that introduces the problem of \textit{overfitting} where the hypothesis fits the training data \textit{too} well and it will not fit the test data.
\citep{norvigAI}

\textit{Model selection} is thus performed by evaluating performance on a \textit{validation set} (sometimes called a \textit{cross-validation set}). The reason not to both choose the model and evaluate the accuracy using the test set is that then we will have overfit the test set as we both choose the best model and then calculate the accuracy \textit{on the already best fit}.
\citep{norvigAI}

In practice you have a set of data which you randomly split into the training, test and validation sets. The recommended ratio differs, common recommendations include 60/20/20, 80/10/10, or 70/15/15 \footnote{As recommended by Andrew Ng, Stanford. \url{https://class.coursera.org/ml-006}} depending on domain and the size of the available data set. The training set is larger than the others.

If there is no need for a validation set, which can be the case if there are no models to choose from, common training/test set ratios include 70/30, 80/20 or 90/10 \cite{hu2008collaborative, norvigAI} \footnote{Andrew Ng also mentions these values}.

In summary machine learning for supervised learning is done in a couple of steps:

\begin{description}
    \item[Preface] Split data set into training, test and validation sets.
    \item[Training phase] Train the hypothesis using the training set.
    \item[Model selection] Select model using the validation set. (Optional)
    \item[Evaluation] Estimate the accuracy using the test set.
    \item[Application] Apply the developed model to real world data and get results.
\end{description}

Another way to combat overfitting is with \textit{regularization}. Regularization searches for a hypothesis directly penalizes complexity.  Regularization still needs to select $\lambda$ using model selection.
\citep{norvigAI}
