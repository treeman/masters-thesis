
\subsection{Optimization}

For ratings there are two state of the art prediction techniques:

\begin{itemize}
    \item ALS - Alternating Least Square
    \item SGD - Stochastic Gradient Descent
\end{itemize}

Both minimizes \rmse.

Gradient descent is an optimization technique in a multi-dimensional function space which walks in the gradient's direction.  This necessitates the use of a differential function.


\subsubsection{Cost functions}

Usually recommendation algorithms have an associated cost function to minimize, but ours don't seem to have one.


\subsubsection{Non-smooth functions}

This is what our functions seems to be?

\url{http://scicomp.stackexchange.com/questions/1506/non-differentiable-global-optimization-problem}

\url{http://www.amazon.com/dp/0898712564/?tag=stackoverfl08-20}

etc...

