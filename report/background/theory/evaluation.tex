\subsection{Evaluation}\label{sec:background:theory:eval}

A common technique to evaluate the accuracy, or the quality of recommendations, as sets is with \textit{Precision}, \textit{Recall} and \textit{F-measure}
\footnote{The 2nd Linked Open Data-enabled Recommender Systems Challenge uses \textit{F-measure}, 2015. \url{http://sisinflab.poliba.it/events/lod-recsys-challenge-2015/}}
 \citep{bobadilla2013recommender}.
Evaluating as sets is done in the evaluation and model selection phase of supervised learning.

To evaluate between sets, let $r_{u, i}$ be the final recommendations in binary form \eqref{eq:binrec} produced by the training set $A_{train}$. It's possible to evaluate Top-$N$ recommendations by simply constraining the recommendation set $r_{u, i}$ \eqref{eq:constrain_N}.
Let $e_{u, i}$ be the interaction history as described by the evaluation set. The evaluation set could either be the test set $A_{test}$ or the validation set $A_{val}$, so $e_{u, i}$ should either represent $A_{test}$ or $A_{val}$.

First define \textit{true positives} $\TP$ as the sum of all correctly predicted positive samples.

\begin{equation} \label{eq:tp}
    \TP = \sum_{u, i} r_{u, i} = 1 \land \, e_{u, i} = 1
\end{equation}

Conversely \textit{false positives} $\FP$ is the sum of all falsely predicted positive samples.

\begin{equation} \label{eq:fp}
    \FP = \sum_{u, i} r_{u, i} = 1 \land \, e_{u, i} = 0
\end{equation}

And \textit{false negatives} $\FN$ is the sum of all falsely predicted negative samples.

\begin{equation} \label{eq:fn}
    \FN = \sum_{u, i} r_{u, i} = 0 \land \, e_{u, i} = 1
\end{equation}

Then \textit{Precision} and \textit{Recall} is defined as

\begin{equation} \label{eq:precision}
    \precision = \frac{\TP}{\TP + \FP}
\end{equation}

\begin{equation} \label{eq:recall}
    \recall = \frac{\TP}{\TP + \FN}
\end{equation}

Loosely \textit{Precision} signifies how well the recommended items correspond to the users' actual preferences as described by the evaluation set and \textit{Recall} signifies how well the users' preferences contained in the evaluation set fits with the recommendations.

In many ways precision and recall are competing measures, when optimizing for precision recall decreases and vice versa.  As the number of recommendations $N$ grow precision is expected to be lower and recall is expected to be higher. \citep{hu2008collaborative}

\textit{F-measure} $\fmeasure$ is defined as the harmonic mean of precision and recall \eqref{eq:f1} as a combined measure of precision and recall.

\begin{equation} \label{eq:f1}
    \fmeasure = \frac{2 * \precision * \recall}{\precision + \recall}
\end{equation}

Another evaluation method commonly used to evaluate classifications with ratings is the Root of Mean Square Error (\texttt{RMSE}).
\citep{bobadilla2013recommender}

\begin{equation} \label{eq:rmse}
    \mathtt{RMSE} = \sqrt{\frac{\sum_{u, i}^n (r_{u, i} - e_{u, i})^2}{n}}
\end{equation}

