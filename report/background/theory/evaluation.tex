
\subsection{Evaluation}\label{sec:background:theory:eval}

A common technique to evaluate the quality of recommendations as sets is with \textit{Precision}, \textit{Recall} and \textit{F-measure}
\footnote{The 2nd Linked Open Data-enabled Recommender Systems Challenge uses \textit{F-measure}, 2015. \url{http://sisinflab.poliba.it/events/lod-recsys-challenge-2015/}}
. \citep{bobadilla2013recommender}

\Warning[TODO]{ Here $h_{u, i}$ isn't the interaction history, but the test/validation set }

First, \textit{true positives}, $TP$, is the sum of all correctly predicted positive samples.

\begin{equation} \label{eq:tp}
    TP = \sum_{u, i} r_{u, i} = 1 \land \, h_{u, i} = 1
\end{equation}

Conversely \textit{false positives}, $FP$, is the sum of all falsely predicted positive samples.

\begin{equation} \label{eq:fp}
    FP = \sum_{u, i} r_{u, i} = 1 \land \, h_{u, i} = 0
\end{equation}

And \textit{false negatives}, $FN$, is the sum of all falsely predicted negative samples.

\begin{equation} \label{eq:fn}
    FN = \sum_{u, i} r_{u, i} = 0 \land \, h_{u, i} = 1
\end{equation}

Then \textit{Precision} $P$ and \textit{Recall} $R$ is defined as

\begin{equation} \label{eq:precision}
    P = \TP / (\TP + \FP)
\end{equation}

\begin{equation} \label{eq:recall}
    R = \TP / (\TP + \FN)
\end{equation}

Loosely \textit{Precision} signifies how well the recommended items correspond to the users' preferences and \textit{Recall} signifies how well the users' preferences fits with the recommendations.

In many ways precision and recall are competing measures, when optimizing for precision recall decreases and vice versa.  As the number of recommendations $N$ grow precision is expected to be lower and recall is expected to be higher. \citep{hu2008collaborative}


\textit{F-measure}, $\mathit{F1}$, is defined as the harmonic mean of precision and recall \eqref{eq:f1} as a combined measure of precision and recall.

\begin{equation} \label{eq:f1}
    \mathit{F1} = 2 * P * R / (P + R)
\end{equation}

%\Warning[TODO]{ Evaluations for top-N }

% TODO remove?
Another evaluation method commonly used to evaluate explicit feedback system with ratings is the Root of Mean Square Error \rmse \hspace{0.2ex} \eqref{eq:rmse} where $\mathit{pval}$ is the predicted ratings for the user-item interaction matrix $h$.
\citep{bobadilla2013recommender}
%\Warning[TODO]{ Conflicting variable names? }

\begin{equation} \label{eq:rmse}
    \mathtt{RMSE} = \sqrt{\mathbf{E}\left\{ (\mathit{pval} - h)^2 \right\}}
\end{equation}
\Warning[TODO]{ Matrices are usually big lettered! }

