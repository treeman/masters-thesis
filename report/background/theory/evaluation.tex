
\subsection{Evaluation}\label{sec:background:theory:eval}

A common technique to evaluate the quality of recommendations as sets is with \textit{Precision}, \textit{Recall} and \textit{F-measure}
\footnote{The 2nd Linked Open Data-enabled Recommender Systems Challenge uses \textit{F-measure}, 2015. \url{http://sisinflab.poliba.it/events/lod-recsys-challenge-2015/}}
. \citep{bobadilla2013recommender}

To evaluate between sets, let $r_{u, i}$ be the recommendations in binary form \eqref{eq:binrec} and let $e_{u, i}$ be the interaction history as described by the evaluation set. Practically it means $e_{u, i}$ could either represent the test set or the validation set.

First, \textit{true positives}, $\TP$, is the sum of all correctly predicted positive samples.

\begin{equation} \label{eq:tp}
    \TP = \sum_{u, i} r_{u, i} = 1 \land \, e_{u, i} = 1
\end{equation}

Conversely \textit{false positives}, $\FP$, is the sum of all falsely predicted positive samples.

\begin{equation} \label{eq:fp}
    \FP = \sum_{u, i} r_{u, i} = 1 \land \, e_{u, i} = 0
\end{equation}

And \textit{false negatives}, $\FN$, is the sum of all falsely predicted negative samples.

\begin{equation} \label{eq:fn}
    \FN = \sum_{u, i} r_{u, i} = 0 \land \, e_{u, i} = 1
\end{equation}

Then \textit{Precision} and \textit{Recall} is defined as

\begin{equation} \label{eq:precision}
    \precision = \frac{\TP}{\TP + \FP}
\end{equation}

\begin{equation} \label{eq:recall}
    \recall = \frac{\TP}{\TP + \FN}
\end{equation}

Loosely \textit{Precision} signifies how well the recommended items correspond to the users' actual preferences as described by the evaluation set and \textit{Recall} signifies how well the users' preferences contained in the evaluation set fits with the recommendations.

In many ways precision and recall are competing measures, when optimizing for precision recall decreases and vice versa.  As the number of recommendations $N$ grow precision is expected to be lower and recall is expected to be higher. \citep{hu2008collaborative}


\textit{F-measure}, denoted $\fmeasure$, is defined as the harmonic mean of precision and recall \eqref{eq:f1} as a combined measure of precision and recall.

\begin{equation} \label{eq:f1}
    \fmeasure = \frac{2 * \precision * \recall}{\precision + \recall}
\end{equation}

Another evaluation method commonly used to evaluate explicit feedback system with ratings is the Root of Mean Square Error (\texttt{RMSE}).
\citep{bobadilla2013recommender}

\begin{equation} \label{eq:rmse}
    \mathtt{RMSE} = \sqrt{\frac{\sum_{u, i}^n (p_{u, i} - e_{u, i})^2}{n}}
\end{equation}

