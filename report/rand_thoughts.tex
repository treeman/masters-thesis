\chapter{Random thoughts}

\textit{Remove later, collect random things.}


\section{Theory}

Describe Collaborative Filtering!

Binary classifier, likes?

Things I need to research more:

\begin{enumerate}
    \item Collaborative filtering, neighbours? \cite{hu2008collaborative}
    \item Matrix factorization.
\end{enumerate}

\section{Related Work}

A common deficiency for evaluation metrics is the lack of formalization. The metrics themselves are well defined but implementation details differ and are sometimes missing which can lead to different results between similar experiments.
\Warning[TODO]{ Move to evaluation chapter? }
\Warning[TODO]{ Comment on strange evaluation results using precision/recall/F1 definitions from another article? }


Both algorithms are nearest neighbour algorithms? Also collaborative filtering? Or?

They are model based algorithms though.

Ignore the cold start problem.

Ignore explaining recommendations.

Different evaluation strategies include: Novelty Precision/Recall. Coverage. Trust Recall.

See \citep{bobadilla2013recommender} for definitions.

\begin{enumerate}
    \item prediction evaluations (accuracy) (Quality of the predictions)

        Such as Mean Absolute Error (MAE), Root of Mean Square Error (RMSE), Normalized Mean Average Error (NMAE)

    \item evaluations for recommendations as sets (precision) (Quality of set of recommendations)

        Precision, Recall, Receiver Operating Characteristic (ROC)

    \item evaluation as ranked lists (Quality of list of recommendations)

        half-life, discounted cumulative gain

    \item diversity metrics
\end{enumerate}

Cross validation techniques:
    random sub-sampling and k-fold cross validation

ESWC 2014 discussions by \citep{di2014linked}, \citep{heitmann2014semstim}, \citep{ostuni2014linked}. Read them!


