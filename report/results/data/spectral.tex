
\subsubsection{Connectivity using Spectral Clustering}

The process for clustering with spectral clustering is as follows
\footnote{A clear explanation of spectral clustering is given by \\
charlesmartin14: Spectral Clustering: A quick overview, 2012. \\
\url{https://charlesmartin14.wordpress.com/2012/10/09/spectral-clustering/}}

\begin{enumerate}
    \item Create an affinity matrix, or an adjacency matrix, $A_f$
    \item Construct a graph Laplacian $L$ from $A_f$
    \item Find eigenvalues and eigenvectors of $L$
    \item Select a subspace of eigenvectors
    \item Form clusters in the subspace
\end{enumerate}

In our particular case, the interaction matrix $A$ is defined as rows corresponding to users and columns corresponding to items. The affinity matrix needs to be square, but the interaction matrix is not. There are other more complex ways of creating an affinity matrix, but for this purpose modelling an adjacency matrix is sufficient.

A transformation from the interaction matrix $A$ to the adjacency matrix $A_{f}$ is made by having both users and items as both row and column indices and mirroring the interactions. $A_{f}$ will then be a symmetric, square matrix. Equation \eqref{eq:make_adj} illustrates an example of the matrix structure and \figureref{fig:spec:book:matrix} gives a concrete example for \textit{eswc2015books}.

\begin{equation}\label{eq:make_adj}
  A = \kbordermatrix{
    &    i_1 & i_2 & i_3 & i_4 \\
    u_1 & 0   & 1   & 0   & 1  \\
    u_2 & 0   & 1   & 1   & 1  \\
    u_3 & 1   & 0   & 1   & 0
  }
  \Rightarrow
    A_f = \kbordermatrix{
        &    u_1 & u_2 & u_3 & i_1 & i_2 & i_3 & i_4 \\
        u_1 & 0   & 0   & 0  &  0  &  1  &  0  &  1  \\
        u_2 & 0   & 0   & 0  &  0  &  1  &  1  &  1  \\
        u_3 & 0   & 0   & 0  &  1  &  0  &  1  &  0 \\
        i_1 & 0   & 0   & 1  &  0  &  0  &  0  &  0 \\
        i_2 & 1   & 1   & 0  &  0  &  0  &  0  &  0 \\
        i_3 & 0   & 1   & 1  &  0  &  0  &  0  &  0 \\
        i_4 & 1   & 1   & 0  &  0  &  0  &  0  &  0 \\
    }
\end{equation}

\begin{figure}[h!]
    \begin{subfigure}[h!]{0.5\textwidth}
        \includegraphics[width=\textwidth]{fig/spectral_data/eswc2015books_original.png}
        \caption{Interaction matrix $A$}
        \label{fig:spec:book:orig}
    \end{subfigure}
    ~
    \begin{subfigure}[h!]{0.5\textwidth}
        \includegraphics[width=\textwidth]{fig/spectral_data/eswc2015books_adj.png}
        \caption{Adjacency matrix $A_f$}
        \label{fig:spec:book:adj}
    \end{subfigure}
    \caption{This figure illustrates the original interaction matrix \ref{fig:spec:book:orig} for \textit{eswc2015books} and the related adjecency matrix \ref{fig:spec:book:adj}. Here the matrix $A$ has been moved to the upper right corner of $A_f$ as well as mirrored at the bottom left.}
    \label{fig:spec:book:matrix}
\end{figure}

There are different kinds of Laplacians, mostly differing in how the normalization is done. The one used here is the Generalized Laplacian $L$.

\begin{equation}
    L = D^{-1}( D - A_f )
\end{equation}

$D$ is a diagonal matrix called the degree matrix. Each diagonal element $D_{i, i}$ represent the sum of degrees at each node $i$, calculated as the sum of row $i$.

\begin{equation}
    D_{i, i} = \sum_j A_{i, j}
\end{equation}

The principal idea is that if good clusters can be identified, then the Laplacian $L$ is approximately block diagonal, with each cluster defined by a block. That is if there are 3 clusters

\begin{equation}
    L =
    \begin{pmatrix}
        L_{1, 1} & L_{1, 2} & L_{1, 3} \\
        L_{2, 1} & L_{2, 2} & L_{2, 3} \\
        L_{3, 1} & L_{3, 2} & L_{3, 3}
    \end{pmatrix}
    \sim
    \begin{pmatrix}
        L_{1, 1} & 0         & 0        \\
        0        & L_{2, 2}  & 0        \\
        0        & 0         & L_{3, 3}
    \end{pmatrix}
\end{equation}

Then also the lowest eigenvalues and their related eigenvectors correspond to different clusters. In this case the 3 smallest eigenvalues and eigenvector pair would correspond to each cluster, or block, in $L$.

To be able to identify different clusters, the sorted eigenvalues must have a gap. \Figureref{fig:spec:book:eigv} gives a concrete example for \textit{eswc2015books} (another clear example is \figureref{fig:spec:alphaS:eigv} for \textit{alphaS}). It is reasonable to expect there to be clear clusters in the adjacency matrix $A_f$, as there is a clear gap in the sorted eigenvalues. Note that this doesn't mean that there are clusters in the interaction matrix $A$, as there is duplicate information in $A_f$. There might be large areas without interactions, but the indexes might correspond to user-user or item-item which will not have any interactions. But it's a reasonable expectation.

The subspace to find clusters in is some subset of the eigenvectors corresponding to the smallest eigenvalues. The subspace used here is simply the eigenvector corresponding to the 2nd smallest eigenvalue. 
\footnote{A practical example which used the same subspace was used as a reference. \\
Spectral Graph Partitioning and the Laplacian with Matlab, 2006. \\
\url{https://www.cs.purdue.edu/homes/dgleich/demos/matlab/spectral/spectral.html}
}
\Figureref{fig:spec:book:eigsort} displays the adjacency matrix $A_f$ for \textit{eswc2015books} ordered by the ordering used when sorting the subspace. There is some kind of structure in the graph, but it's hard to identify what it represents. The expectation is to find clusters represented by squares along the diagonal and there are some outlines of squares along the diagonal.

\FloatBarrier

\twodiffpiclabel{fig/spectral_data/eswc2015books_eigv.png}
{The smallest eigenvalues of $L$ for \textit{eswc2015books}.}
{fig:spec:book:eigv}
{fig/spectral_data/eswc2015books_eig_sort.png}
{Adjacency matrix $A$ of \textit{eswc2015books}, sorted by the ordering used when sorting the 2nd smallest eigenvector of $L$.}
{fig:spec:book:eigsort}

\FloatBarrier

If instead of simply sorting the subspace, \textit{k-means} is used to find a clustering in the subspace, and that ordering is then used to reorder the adjacency matrix. \Figureref{fig:spec:book:kmeans} displays the adjacency matrix $A_f$ received for \textit{eswc2015books}. There are several, apparent, clusters visible. This doesn't directly mean that there are clusters in the dataset, but the same clustering information was used to reveal clusters in the original interaction matrix $A$, visible in \figureref{fig:spec:book:clust}.

In contrast with the compactness clustering which only clustered on a user level, this time there is clustering information for both users and items. This is a substantial benefit and some very clear clusters are revealed for \textit{eswc2015books}.

\FloatBarrier

\twodiffpiclabel{fig/spectral_data/eswc2015books_kmeans_sort.png}
{Adjacency matrix $A_f$ of \textit{eswc2015books}, subspace clustered with \textit{k-means}.}
{fig:spec:book:kmeans}
{fig/spectral_data/eswc2015books_spectral_clust.png}
{Interaction matrix $A$ of \textit{eswc2015books}, reordered using \textit{k-means} clustering information.}
{fig:spec:book:clust}

\FloatBarrier

What follows is a similar analysis for the other datasets.

\FloatBarrier

\twodiffpiclabel{fig/spectral_data/alphaS_eigv.png}
{The smallest eigenvalues of $L$ for \textit{alphaS}.}
{fig:spec:alphaS:eigv}
{fig/spectral_data/alphaS_eig_sort.png}
{Adjacency matrix $A$ of \textit{alphaS}, sorted by the subspace ordering.}
{fig:spec:alphaS:eigsort}

\twodiffpiclabel{fig/spectral_data/alphaS_kmeans_sort.png}
{Adjacency matrix $A_f$ of \textit{alphaS}, subspace clustered with \textit{k-means}.}
{fig:spec:alphaS:kmeans}
{fig/spectral_data/alphaS_spectral_clust.png}
{Interaction matrix $A$ of \textit{alphaS}, reordered using \textit{k-means} clustering information.}
{fig:spec:alphaS:clust}

\FloatBarrier

\textit{alphaS} have many very prominent clusters which is evident in the clustered interaction matrix in \figureref{fig:spec:alphaS:clust}. This is supported by the large number of small eigenvalues in \figureref{fig:spec:alphaS:eigv} preceding a gap and also visible even without resorting to clustering but simply sorting the subspace in \figureref{fig:spec:alphaS:eigsort}. From the clustering made here it's possible to identify groups of users with specialized interests, with a subset of appealing items. This could possibly be used to identify personas
\footnote{Which is a sought-after feature from Commordo's e-commerce clients. This process however is not automated and it's just a bi-product of this thesis and it's not pursued further.}
in the dataset.

The adjacency matrices for the following datasets are not plotted and only the eigenvalues and the final clustering of the interaction matrix $A$ is presented. They are sufficient to draw conclusions from.

\FloatBarrier

\twodiffpiclabel{fig/spectral_data/eswc2015movies_eigv.png}
{The smallest eigenvalues of $L$ for \textit{eswc2015movies}.}
{fig:spec:eswc2015movies:eigv}
{fig/spectral_data/eswc2015movies_spectral_clust.png}
{Interaction matrix $A$ of \textit{eswc2015movies}, reordered using \textit{k-means} clustering information.}
{fig:spec:eswc2015movies:clust}

\twodiffpiclabel{fig/spectral_data/eswc2015music_eigv.png}
{The smallest eigenvalues of $L$ for \textit{eswc2015music}.}
{fig:spec:eswc2015music:eigv}
{fig/spectral_data/eswc2015music_spectral_clust.png}
{Interaction matrix $A$ of \textit{eswc2015music}, reordered using \textit{k-means} clustering information.}
{fig:spec:eswc2015music:clust}

\twodiffpiclabel{fig/spectral_data/movielens1m_eigv.png}
{The smallest eigenvalues of $L$ for \textit{movielens1m}.}
{fig:spec:movielens1m:eigv}
{fig/spectral_data/movielens1m_spectral_clust.png}
{Interaction matrix $A$ of \textit{movielens1m}, reordered using \textit{k-means} clustering information.}
{fig:spec:movielens1m:clust}

\FloatBarrier

\twodiffpiclabel{fig/spectral_data/romeo_eigv.png}
{The smallest eigenvalues of $L$ for \textit{romeo}.}
{fig:spec:romeo:eigv}
{fig/spectral_data/romeo_spectral_clust.png}
{Interaction matrix $A$ of \textit{romeo}, reordered using \textit{k-means} clustering information.}
{fig:spec:romeo:clust}

Clusters are visible in the final clustering of the interaction matrix $A$ for all datasets. It's supported by their respective eigenvalue plot, with all datasets have a prominent gap. \textit{eswc2015movies}, \textit{eswc2015music} and \textit{movielens1m} have clusters but they are messier than that of \textit{romeo} or \textit{alphaS}. Part of the reason is the visualization technique where a sparse interaction matrix will produce cleaner visualizations.

This analysis searched for a fixed number of clusters $k = 10$, with \textit{alphaS} using $k = 15$. It is by no means the optimal number of clusters and there might be more clusters in the datasets and there might fewer but ``better fitting'' clusters. The point of this analysis was not to cluster the datasets in an optimal way, but to examine if the datasets have any clusters, which can be said that they have.

