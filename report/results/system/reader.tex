
\subsection{Reader module}

The reader module takes data files, with client specific formatting, and stores the data in the databases. The data contains user interaction history of some sort, possibly as a list of user-item pairs, but it can also contain additional user and item information all in a single file or in several.

To allow for flexibility the reader module uses a plugin system which can be selected at runtime. This is accomplished using python's dynamic module loading capabilities.

Firstly the reader module will get a list of available plugins found in

\begin{lstlisting}
    lib/reader_plugins
\end{lstlisting}

The plugin class shall have a single uppercase letter and the rest lowercase and reside in a file with all lowercase. For example a plugin which handles \textit{eswc} data could have the class ``Eswc'' inside a file ``eswc.py'' in the plugin directory.

Secondly the appropriate plugin will be selected via command line arguments and the plugin class will be handed control. The class should have two methods: ``add\_arguments'' which parses extra command line arguments and ``load'' which shall return a user hash and a product hash. \Appendixref{sec:eswc_plugin} describes a full example plugin which handles \textit{eswc} data.

With the selected data the reader module can then generate matlab data file output in the form of a ``.mat'' file, upload the data to the database or simply print some statistics. When generating a ``.mat'' file different ratios of training, validation and test sets can be set. The purpose of this option is to generate datasets used during prototyping and evaluation.

The reader module can remove items and users from the dataset by introducing a couple of constraints:

\begin{enumerate}
    \item limit the maximum number of users in the dataset
    \item limit the maximum number of items in the dataset
    \item remove users with too few item interactions in their history
    \item remove items which too few users has interacted with
\end{enumerate}

The reason to limit the size of the dataset is due to the high computational complexity and the bad performance on large datasets. The removal of items or users with too few interactions is because of the difficulty of generating recommendations for items or users with no history. This is known as the \textit{cold start problem} and it's a known difficulty in recommendation systems \citep{cacheda2011comparison}.

The reader module tries to comform to the constraints with these steps:

\begin{enumerate}
    \item \label{filter_users} Remove users with too few items in history, if required to
    \item \label{filter_items} Remove items which too few users has interacted with, if required to

    \item Limit the number of items, if required
        \begin{enumerate}
            \item Randomly select the items to keep
            %\item Remove the removed items from the user history
        \end{enumerate}
    \item Limit the number of users, if required
        \begin{enumerate}
            \item Randomly select the users to keep
        \end{enumerate}

    \item Perform step \ref{filter_users} again
    \item Perform step \ref{filter_items} again
\end{enumerate}

This will not produce a perfect solution and some constraints may not be fulfilled. If we for example want to want to constrain both the minimum number of item interactions each user has and the minimum number of user interaction each item has, we might fail to find a solution as the removal of some items may cause some users to have fewer than the constrained number of item interactions.

The alternative is to introduce a constraint solver or iteratively perform step \ref{filter_users} and \ref{filter_items} until convergence, but that's a slow solution to a problem with inherently soft constraints. It is not very important if \textit{all} constraints hold, it is just an attempt to limit the size of the dataset. Therefore a faster but less correct solution was chosen.

