

\section{Methodology}\label{sec:disc:method}

\textit{Discuss and critique for the methods chosen.}

\textit{Also discuss sources.}

The usage of the \textit{movielens1m} dataset can be questioned as the dataset is made up by ratings and then converted to unweighted binary form. This introduces a lot of noise and the question is what conclusions can be drawn from such a dataset.

The evaluation metric of \textit{F-measure} can be questioned/discussed...? Why no cost function? Probably would be better (if could be found, but can it?) ?

Alternative evaluation metrics. \rmse, rank, accuracy, ...?

Optimization technique interesting to try is bayesian optimization! (can we try it?)

Clustering would be nice to explore (if we don't do it in the thesis).

A common deficiency for evaluation metrics is the lack of formalization. The metrics themselves are well defined but implementation details differ and are sometimes missing which can lead to different results between similar experiments.
